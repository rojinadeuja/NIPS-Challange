{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "**PROCESS**\n",
    "For our feature selection model we use the following methods:\n",
    "1. Lasso Regularization\n",
    "2. Tree-based Classification (Random Tree Classifier with Gradient Boosting)\n",
    "3. Step Forward Selection (SFS)\n",
    "4. Exhasutive Forwars Selection (EFS)l\n",
    "\n",
    "Before starting on our feature selection model, we also use the sklearn.feature_selection_SelectKBest model as a baseline. We evaluate the performance of this libarary on the dataset with k= 20, since we know that there are 20 combinational useful features. The accuracy of the model to predict the final set of feature set is noted with the help of our benchmark model ie. Decision Tree Classifier.\n",
    "\n",
    "\n",
    "\n",
    "**RESULTS**\n",
    "We combine the two methods with the highest accuracy (Tree-based Classification and Step Forward Selection) to carry out a voting of features. The features that are voted by both the methods are moved forward for further processing. As a part of the final task, we carry out an Exhasutive forward selection (Greedy search) that checks each combination of features to find out the highest accuracy that can be achieved. From this, we finally get our 5 true features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel, SelectKBest, RFE, chi2, f_regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V492</th>\n",
       "      <th>V493</th>\n",
       "      <th>V494</th>\n",
       "      <th>V495</th>\n",
       "      <th>V496</th>\n",
       "      <th>V497</th>\n",
       "      <th>V498</th>\n",
       "      <th>V499</th>\n",
       "      <th>V500</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>485</td>\n",
       "      <td>477</td>\n",
       "      <td>537</td>\n",
       "      <td>479</td>\n",
       "      <td>452</td>\n",
       "      <td>471</td>\n",
       "      <td>491</td>\n",
       "      <td>476</td>\n",
       "      <td>475</td>\n",
       "      <td>473</td>\n",
       "      <td>...</td>\n",
       "      <td>481</td>\n",
       "      <td>477</td>\n",
       "      <td>485</td>\n",
       "      <td>511</td>\n",
       "      <td>485</td>\n",
       "      <td>481</td>\n",
       "      <td>479</td>\n",
       "      <td>475</td>\n",
       "      <td>496</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>483</td>\n",
       "      <td>458</td>\n",
       "      <td>460</td>\n",
       "      <td>487</td>\n",
       "      <td>587</td>\n",
       "      <td>475</td>\n",
       "      <td>526</td>\n",
       "      <td>479</td>\n",
       "      <td>485</td>\n",
       "      <td>469</td>\n",
       "      <td>...</td>\n",
       "      <td>478</td>\n",
       "      <td>487</td>\n",
       "      <td>338</td>\n",
       "      <td>513</td>\n",
       "      <td>486</td>\n",
       "      <td>483</td>\n",
       "      <td>492</td>\n",
       "      <td>510</td>\n",
       "      <td>517</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>487</td>\n",
       "      <td>542</td>\n",
       "      <td>499</td>\n",
       "      <td>468</td>\n",
       "      <td>448</td>\n",
       "      <td>471</td>\n",
       "      <td>442</td>\n",
       "      <td>478</td>\n",
       "      <td>480</td>\n",
       "      <td>477</td>\n",
       "      <td>...</td>\n",
       "      <td>481</td>\n",
       "      <td>492</td>\n",
       "      <td>650</td>\n",
       "      <td>506</td>\n",
       "      <td>501</td>\n",
       "      <td>480</td>\n",
       "      <td>489</td>\n",
       "      <td>499</td>\n",
       "      <td>498</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>480</td>\n",
       "      <td>491</td>\n",
       "      <td>510</td>\n",
       "      <td>485</td>\n",
       "      <td>495</td>\n",
       "      <td>472</td>\n",
       "      <td>417</td>\n",
       "      <td>474</td>\n",
       "      <td>502</td>\n",
       "      <td>476</td>\n",
       "      <td>...</td>\n",
       "      <td>480</td>\n",
       "      <td>474</td>\n",
       "      <td>572</td>\n",
       "      <td>454</td>\n",
       "      <td>469</td>\n",
       "      <td>475</td>\n",
       "      <td>482</td>\n",
       "      <td>494</td>\n",
       "      <td>461</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>484</td>\n",
       "      <td>502</td>\n",
       "      <td>528</td>\n",
       "      <td>489</td>\n",
       "      <td>466</td>\n",
       "      <td>481</td>\n",
       "      <td>402</td>\n",
       "      <td>478</td>\n",
       "      <td>487</td>\n",
       "      <td>468</td>\n",
       "      <td>...</td>\n",
       "      <td>479</td>\n",
       "      <td>452</td>\n",
       "      <td>435</td>\n",
       "      <td>486</td>\n",
       "      <td>508</td>\n",
       "      <td>481</td>\n",
       "      <td>504</td>\n",
       "      <td>495</td>\n",
       "      <td>511</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    V1   V2   V3   V4   V5   V6   V7   V8   V9  V10  ...  V492  V493  V494  \\\n",
       "0  485  477  537  479  452  471  491  476  475  473  ...   481   477   485   \n",
       "1  483  458  460  487  587  475  526  479  485  469  ...   478   487   338   \n",
       "2  487  542  499  468  448  471  442  478  480  477  ...   481   492   650   \n",
       "3  480  491  510  485  495  472  417  474  502  476  ...   480   474   572   \n",
       "4  484  502  528  489  466  481  402  478  487  468  ...   479   452   435   \n",
       "\n",
       "   V495  V496  V497  V498  V499  V500  Class  \n",
       "0   511   485   481   479   475   496      2  \n",
       "1   513   486   483   492   510   517      2  \n",
       "2   506   501   480   489   499   498      2  \n",
       "3   454   469   475   482   494   461      1  \n",
       "4   486   508   481   504   495   511      1  \n",
       "\n",
       "[5 rows x 501 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Load and quick check the data'''\n",
    "df= pd.read_csv(\"medilon.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['Class'], axis = 1)\n",
    "y = df['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. SelectKBest\n",
    "Firstly, we use the sklearn's SelectKBest for selecting our prominent features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "skb = SelectKBest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_KBest(data):\n",
    "    '''\n",
    "    Find the 20 best features from a set of data. Data input should come without a target column.\n",
    "    '''\n",
    "    for col in data.columns:\n",
    "        \n",
    "        X = data.drop([col], axis = 1)\n",
    "        y = data[col]\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "        \n",
    "        fit = SelectKBest(score_func= f_regression, k=20).fit(X_train, y_train)\n",
    "    \n",
    "        skb_feats = list(np.where(fit.get_support())[0])\n",
    "        \n",
    "    return skb_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "skb_feats = find_KBest(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of features selected are: 20\n",
      "The selected features are: [12, 34, 76, 95, 119, 141, 183, 293, 307, 350, 351, 361, 364, 383, 389, 397, 424, 430, 451, 488]\n"
     ]
    }
   ],
   "source": [
    "print('The number of features selected are:', len(skb_feats))\n",
    "print('The selected features are:', skb_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V13</th>\n",
       "      <th>V35</th>\n",
       "      <th>V77</th>\n",
       "      <th>V96</th>\n",
       "      <th>V120</th>\n",
       "      <th>V142</th>\n",
       "      <th>V184</th>\n",
       "      <th>V294</th>\n",
       "      <th>V308</th>\n",
       "      <th>V351</th>\n",
       "      <th>V352</th>\n",
       "      <th>V362</th>\n",
       "      <th>V365</th>\n",
       "      <th>V384</th>\n",
       "      <th>V390</th>\n",
       "      <th>V398</th>\n",
       "      <th>V425</th>\n",
       "      <th>V431</th>\n",
       "      <th>V452</th>\n",
       "      <th>V489</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1593</th>\n",
       "      <td>494</td>\n",
       "      <td>471</td>\n",
       "      <td>482</td>\n",
       "      <td>486</td>\n",
       "      <td>502</td>\n",
       "      <td>552</td>\n",
       "      <td>468</td>\n",
       "      <td>480</td>\n",
       "      <td>483</td>\n",
       "      <td>473</td>\n",
       "      <td>447</td>\n",
       "      <td>501</td>\n",
       "      <td>479</td>\n",
       "      <td>486</td>\n",
       "      <td>464</td>\n",
       "      <td>449</td>\n",
       "      <td>489</td>\n",
       "      <td>494</td>\n",
       "      <td>466</td>\n",
       "      <td>465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>497</td>\n",
       "      <td>482</td>\n",
       "      <td>477</td>\n",
       "      <td>447</td>\n",
       "      <td>497</td>\n",
       "      <td>476</td>\n",
       "      <td>493</td>\n",
       "      <td>477</td>\n",
       "      <td>472</td>\n",
       "      <td>481</td>\n",
       "      <td>560</td>\n",
       "      <td>491</td>\n",
       "      <td>483</td>\n",
       "      <td>481</td>\n",
       "      <td>511</td>\n",
       "      <td>442</td>\n",
       "      <td>477</td>\n",
       "      <td>454</td>\n",
       "      <td>482</td>\n",
       "      <td>480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>526</td>\n",
       "      <td>496</td>\n",
       "      <td>476</td>\n",
       "      <td>483</td>\n",
       "      <td>496</td>\n",
       "      <td>507</td>\n",
       "      <td>421</td>\n",
       "      <td>479</td>\n",
       "      <td>482</td>\n",
       "      <td>488</td>\n",
       "      <td>470</td>\n",
       "      <td>475</td>\n",
       "      <td>488</td>\n",
       "      <td>469</td>\n",
       "      <td>569</td>\n",
       "      <td>539</td>\n",
       "      <td>483</td>\n",
       "      <td>466</td>\n",
       "      <td>488</td>\n",
       "      <td>502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2126</th>\n",
       "      <td>453</td>\n",
       "      <td>453</td>\n",
       "      <td>490</td>\n",
       "      <td>525</td>\n",
       "      <td>482</td>\n",
       "      <td>522</td>\n",
       "      <td>466</td>\n",
       "      <td>482</td>\n",
       "      <td>488</td>\n",
       "      <td>475</td>\n",
       "      <td>503</td>\n",
       "      <td>480</td>\n",
       "      <td>480</td>\n",
       "      <td>506</td>\n",
       "      <td>437</td>\n",
       "      <td>511</td>\n",
       "      <td>503</td>\n",
       "      <td>461</td>\n",
       "      <td>473</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1587</th>\n",
       "      <td>444</td>\n",
       "      <td>488</td>\n",
       "      <td>476</td>\n",
       "      <td>493</td>\n",
       "      <td>451</td>\n",
       "      <td>449</td>\n",
       "      <td>474</td>\n",
       "      <td>486</td>\n",
       "      <td>463</td>\n",
       "      <td>483</td>\n",
       "      <td>484</td>\n",
       "      <td>475</td>\n",
       "      <td>486</td>\n",
       "      <td>499</td>\n",
       "      <td>523</td>\n",
       "      <td>483</td>\n",
       "      <td>508</td>\n",
       "      <td>482</td>\n",
       "      <td>484</td>\n",
       "      <td>507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      V13  V35  V77  V96  V120  V142  V184  V294  V308  V351  V352  V362  \\\n",
       "1593  494  471  482  486   502   552   468   480   483   473   447   501   \n",
       "196   497  482  477  447   497   476   493   477   472   481   560   491   \n",
       "239   526  496  476  483   496   507   421   479   482   488   470   475   \n",
       "2126  453  453  490  525   482   522   466   482   488   475   503   480   \n",
       "1587  444  488  476  493   451   449   474   486   463   483   484   475   \n",
       "\n",
       "      V365  V384  V390  V398  V425  V431  V452  V489  \n",
       "1593   479   486   464   449   489   494   466   465  \n",
       "196    483   481   511   442   477   454   482   480  \n",
       "239    488   469   569   539   483   466   488   502  \n",
       "2126   480   506   437   511   503   461   473   500  \n",
       "1587   486   499   523   483   508   482   484   507  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_kbest = X_train.iloc[:, skb_feats]\n",
    "X_test_kbest = X_test.iloc[:, skb_feats]\n",
    "X_test_kbest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing accuracy for the SelectKBest Model is,  0.49230769230769234\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler().fit(X_train_kbest)\n",
    "X_train_kbest = scaler.transform(X_train_kbest)\n",
    "\n",
    "scaler = StandardScaler().fit(X_test_kbest)\n",
    "X_test_kbest = scaler.transform(X_test_kbest)\n",
    "\n",
    "clfr = DecisionTreeClassifier(random_state=42)\n",
    "clfr.fit(X_train_kbest, y_train)\n",
    "\n",
    "train_score = clfr.score(X_train_kbest, y_train)\n",
    "test_score = clfr.score(X_test_kbest, y_test)\n",
    "pred = clfr.predict(X_test_kbest)\n",
    "print('The testing accuracy for the SelectKBest Model is, ', test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The problem with SelectKBest is we need to guess the value of \"k\" to get the number of relevant features right. To overcome this limitation, we need a more sophisticated approach to filter out the noisy data.\n",
    "\n",
    "- Recursive Feature Elimination models the data based on the given estimator to pick out the best features, and order them according to the highest accuracy.\n",
    "\n",
    "- We can control the regularization using regression based algorithm with penalty terms. For this method, we use the Lasso or the L1 regularization since it sets the value of undesired features to zero and removes them from the dataset.\n",
    "\n",
    "- Another approach would be to use a non-linear estimator such as a Random Forest Classifier, since it can look past the noise present in the dataset. It generates multiple hundred trees that contain some degree of randomization which helps to escape the correlations between similar trees. Random forest overcomes such bias that might be present when using only a single estimator like a Decision Tree Classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stepwise Regression (Lasso -L1)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rojin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0003\n",
      "Selected features: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rojin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00031\n",
      "Selected features: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rojin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00032\n",
      "Selected features: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rojin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00033000000000000005\n",
      "Selected features: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rojin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0003400000000000001\n",
      "Selected features: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rojin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0003500000000000001\n",
      "Selected features: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rojin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00036000000000000013\n",
      "Selected features: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rojin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00037000000000000016\n",
      "Selected features: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rojin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0003800000000000002\n",
      "Selected features: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rojin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0003900000000000002\n",
      "Selected features: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rojin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00040000000000000024\n",
      "Selected features: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rojin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00041000000000000026\n",
      "Selected features: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rojin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0004200000000000003\n",
      "Selected features: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rojin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0004300000000000003\n",
      "Selected features: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rojin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00044000000000000034\n",
      "Selected features: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rojin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00045000000000000037\n",
      "Selected features: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rojin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0004600000000000004\n",
      "Selected features: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rojin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0004700000000000004\n",
      "Selected features: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rojin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00048000000000000045\n",
      "Selected features: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rojin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0004900000000000004\n",
      "Selected features: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rojin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005000000000000004\n",
      "Selected features: 15\n"
     ]
    }
   ],
   "source": [
    "'''Hyperparameter tuning to find optimal C'''\n",
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "cs_ = []\n",
    "scores_ = []\n",
    "for c in np.arange(0.0003, 0.0005, 0.00001):\n",
    "    '''We use a Logistic Regression model with the penalty set to Lasso (L1) regularization.\n",
    "    Then we use selectFromModel from sklearn, which will select the features whose coefficients are non-zero.'''\n",
    "    sel_ = SelectFromModel(estimator=LogisticRegression( C=c , penalty='l1'))\n",
    "    sel_.fit(X_train, y_train)\n",
    "    selected_feat = X_train.columns[(sel_.get_support())] #Make a list of with selected features\n",
    "    print(c)\n",
    "    print('Selected features: {}'.format(len(selected_feat)))\n",
    "    X_train_selected = sel_.transform(X_train)\n",
    "    X_test_selected = sel_.transform(X_test)\n",
    "    clfr = DecisionTreeClassifier(random_state=42)\n",
    "    clfr.fit(X_train_selected, y_train)\n",
    "    test_score = clfr.score(X_test_selected, y_test)\n",
    "    cs_.append(c)\n",
    "    scores_.append(test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal value of parameter C is 0.00036000000000000013 0.7942307692307692\n",
      "Highest accuracy achieved is, 0.7942307692307692\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "index, value = max(enumerate(scores_), key=operator.itemgetter(1))\n",
    "index, value\n",
    "print(\"The optimal value of parameter C is\", cs_[index], scores_[index])\n",
    "print(\"Highest accuracy achieved is,\", scores_[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rojin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=LogisticRegression(C=0.00036, class_weight=None,\n",
       "                                             dual=False, fit_intercept=True,\n",
       "                                             intercept_scaling=1, l1_ratio=None,\n",
       "                                             max_iter=100, multi_class='warn',\n",
       "                                             n_jobs=None, penalty='l1',\n",
       "                                             random_state=None, solver='warn',\n",
       "                                             tol=0.0001, verbose=0,\n",
       "                                             warm_start=False),\n",
       "                max_features=None, norm_order=1, prefit=False, threshold=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "'''We use a Logistic Regression model with the penalty set to Lasso (L1) regularization.\n",
    "Then we use selectFromModel from sklearn, which will select the features whose coefficients are non-zero.'''\n",
    "sel_ = SelectFromModel(estimator=LogisticRegression(C=0.00036, penalty='l1')) #0.00036 0.000659\n",
    "sel_.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False,  True, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False,  True, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False,  True, False,  True, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "        True, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False,  True, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False,  True, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False,  True, False,\n",
       "       False, False, False, False, False])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''We see which features were kept (True) and which were discarded (False) in the resulting array'''\n",
    "sel_.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features: 500\n",
      "Selected features: 8\n",
      "Features with coefficients shrunk to 0: 492\n",
      "Index(['V222', 'V286', 'V337', 'V339', 'V379', 'V443', 'V476', 'V494'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "selected_feat = X_train.columns[(sel_.get_support())] #Make a list of with selected features\n",
    "print('Total features: {}'.format((X_train.shape[1])))\n",
    "print('Selected features: {}'.format(len(selected_feat)))\n",
    "print('Features with coefficients shrunk to 0: {}'.format(\n",
    "      np.sum(sel_.estimator_.coef_ == 0)))\n",
    "print(selected_feat) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(Selected_features):\n",
    "    Selected_indexes = []\n",
    "    for feat in Selected_features:\n",
    "        val = feat[1:]\n",
    "        index = int(val) -1\n",
    "        Selected_indexes.append(index)\n",
    "    return Selected_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[221, 285, 336, 338, 378, 442, 475, 493]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Selected_features = list(selected_feat) #List the selected features\n",
    "Selected_indexes_Lasso = get_index(Selected_features) ##[28, 48, 336, 338, 493]\n",
    "Selected_indexes_Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2080, 8), (520, 8))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Keep only the selected features in the dataset'''\n",
    "X_train_selected = sel_.transform(X_train)\n",
    "X_test_selected = sel_.transform(X_test)\n",
    "X_train_selected.shape, X_test_selected.shape #Check if features removed successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Use our benchmark model ie. DecisionTreeClassifier for prediction'''\n",
    "clfr = DecisionTreeClassifier(random_state=42)\n",
    "clfr.fit(X_train_selected, y_train)\n",
    "train_score = clfr.score(X_train_selected, y_train)\n",
    "test_score = clfr.score(X_test_selected, y_test)\n",
    "pred = clfr.predict(X_test_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy from Lasso regression method is 0.7942307692307692\n"
     ]
    }
   ],
   "source": [
    "print('The accuracy from Lasso regression method is', test_score) #Print accuracy on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hence, with Lasso regression we get accuracy (Decision Tree): 0.7942307692307692\n",
      "The selected features are: [221, 285, 336, 338, 378, 442, 475, 493]\n"
     ]
    }
   ],
   "source": [
    "'''Conclusion'''\n",
    "print('Hence, with Lasso regression we get accuracy (Decision Tree):', test_score)\n",
    "print('The selected features are:', Selected_indexes_Lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tree-based Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Load data'''\n",
    "X = df.drop(['Class'], axis = 1)\n",
    "y = df['Class']\n",
    "\n",
    "scaler= StandardScaler().fit(X)\n",
    "X = scaler.transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''Impurity-based feature importance of RandomForestCClassifier'''\n",
    "rfc = RandomForestClassifier(n_estimators = 100, random_state = 0, n_jobs = -1)\n",
    " \n",
    "feat_labels = X.columns.values\n",
    "\n",
    "rfc.fit(X, y)\n",
    "\n",
    "importances = rfc.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "std = np.std([tree.feature_importances_ for tree in rfc.estimators_],\n",
    "                 axis=0)\n",
    "zipped = zip(feat_labels, importances)\n",
    "\n",
    "sorted_list = sorted(zipped, key=lambda x: x[1])\n",
    "gini_values = sorted_list[::-1]\n",
    "\n",
    "a_rfc = gini_values[:20]\n",
    "\n",
    "top_n = 20\n",
    "indices = indices[0:top_n]\n",
    "plt.figure(figsize = (20,10))\n",
    "plt.title(\"Feature importances top %d\" % top_n)\n",
    "plt.bar(range(top_n), importances[indices], \n",
    "        color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(top_n), indices)\n",
    "plt.xlim([-1,top_n])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''Features selected on the basis of their learning capability to predict the target'''\n",
    "gbc = GradientBoostingClassifier(n_estimators=100, random_state = 0)\n",
    " \n",
    "feat_labels = X.columns.values\n",
    "\n",
    "gbc.fit(X, y)\n",
    "\n",
    "importances = gbc.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "std = np.std([tree[0].feature_importances_ for tree in gbc.estimators_],\n",
    "                 axis=0)\n",
    "zipped = zip(feat_labels, importances)\n",
    "\n",
    "sorted_list = sorted(zipped, key=lambda x: x[1])\n",
    "gini_values = sorted_list[::-1]\n",
    "\n",
    "a_gbc = gini_values[:20]\n",
    "\n",
    "top_n = 20\n",
    "indices = indices[0:top_n]\n",
    "plt.figure(figsize = (20,10))\n",
    "plt.title(\"Feature importances top %d\" % top_n)\n",
    "plt.bar(range(top_n), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(top_n), indices)\n",
    "plt.xlim([-1,top_n])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(338, 0.13382703595381673),\n",
       " (475, 0.12035663976046118),\n",
       " (48, 0.07435055896717277),\n",
       " (153, 0.06997644678731912),\n",
       " (378, 0.06369234686883314),\n",
       " (318, 0.060880832344004154),\n",
       " (28, 0.05359550539416976),\n",
       " (105, 0.0508352272728366),\n",
       " (442, 0.03738264032370535),\n",
       " (128, 0.034441704351852745),\n",
       " (241, 0.025601017396147605),\n",
       " (281, 0.022888182467441866),\n",
       " (451, 0.02283670667037866),\n",
       " (64, 0.0184644387914576),\n",
       " (453, 0.01724383648491406),\n",
       " (336, 0.017069051999870417),\n",
       " (493, 0.016938738290663963),\n",
       " (472, 0.01688432814465833)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Adding the values of the impurity based feature importance and its capability to learn'''\n",
    "index_of_added = []\n",
    "added_importance = []\n",
    "for i , j in a_rfc:\n",
    "    for k, l in a_gbc:\n",
    "        if i == k:\n",
    "            index_of_added.append(i)\n",
    "            added_importance.append(j+l)\n",
    "zipped = zip(index_of_added, added_importance)\n",
    "# for feature in zip(index_of_added, added_importance):\n",
    "#     print(feature)\n",
    "    \n",
    "sorted_added_list = sorted(zipped, key=lambda x: x[1])[::-1]\n",
    "sorted_added_list #(Feature, Leaning_capability); sorted by Learning capability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[28,\n",
       " 48,\n",
       " 64,\n",
       " 105,\n",
       " 128,\n",
       " 153,\n",
       " 241,\n",
       " 281,\n",
       " 318,\n",
       " 336,\n",
       " 338,\n",
       " 378,\n",
       " 442,\n",
       " 451,\n",
       " 453,\n",
       " 472,\n",
       " 475,\n",
       " 493]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Get only features index from above list'''\n",
    "Selected_indexes_Tree=[]\n",
    "for i in range(len(sorted_added_list)):\n",
    "    Selected_indexes_Tree.append(sorted_added_list[i][0])\n",
    "Selected_indexes_Tree = sorted(Selected_indexes_Tree)\n",
    "Selected_indexes_Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with top 10 features:  0.801923076923077\n"
     ]
    }
   ],
   "source": [
    "'''Acurracy obtained by taking top 10 features'''\n",
    "chosendf = X.iloc[:, Selected_indexes_Tree[:10]]\n",
    "\n",
    "Rf_train, Rf_test, y_train, y_test = train_test_split(chosendf, y, test_size=0.2, random_state=42)\n",
    "rfc = DecisionTreeClassifier(random_state = 42)\n",
    "rfc.fit(Rf_train, y_train)\n",
    "rfc_predict = rfc.predict(Rf_test)\n",
    "\n",
    "print(\"Accuracy with top 10 features: \", accuracy_score(y_test, rfc_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  Step Forward Selection (SFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:    4.9s finished\n",
      "\n",
      "[2019-12-09 14:49:54] Features: 1/20 -- score: 0.5687704665498592[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done 384 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=-1)]: Done 484 out of 499 | elapsed:    6.3s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 499 out of 499 | elapsed:    6.3s finished\n",
      "\n",
      "[2019-12-09 14:50:01] Features: 2/20 -- score: 0.5817632746138399[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done 384 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=-1)]: Done 498 out of 498 | elapsed:    6.3s finished\n",
      "\n",
      "[2019-12-09 14:50:08] Features: 3/20 -- score: 0.6721101235154773[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done 384 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=-1)]: Done 497 out of 497 | elapsed:    6.5s finished\n",
      "\n",
      "[2019-12-09 14:50:15] Features: 4/20 -- score: 0.7293393299559882[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done 384 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=-1)]: Done 496 out of 496 | elapsed:    6.7s finished\n",
      "\n",
      "[2019-12-09 14:50:22] Features: 5/20 -- score: 0.8005136906656743[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done 236 tasks      | elapsed:    4.9s\n",
      "[Parallel(n_jobs=-1)]: Done 480 out of 495 | elapsed:    7.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 495 out of 495 | elapsed:    7.2s finished\n",
      "\n",
      "[2019-12-09 14:50:30] Features: 6/20 -- score: 0.8106006024715045[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=-1)]: Done 236 tasks      | elapsed:    5.3s\n",
      "[Parallel(n_jobs=-1)]: Done 494 out of 494 | elapsed:    7.8s finished\n",
      "\n",
      "[2019-12-09 14:50:38] Features: 7/20 -- score: 0.8163633666573071[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    3.4s\n",
      "[Parallel(n_jobs=-1)]: Done 228 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=-1)]: Done 478 out of 493 | elapsed:    8.7s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 493 out of 493 | elapsed:    8.9s finished\n",
      "\n",
      "[2019-12-09 14:50:48] Features: 8/20 -- score: 0.8192526137300804[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done 164 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=-1)]: Done 492 out of 492 | elapsed:   10.0s finished\n",
      "\n",
      "[2019-12-09 14:50:58] Features: 9/20 -- score: 0.8201984435678671[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    5.2s\n",
      "[Parallel(n_jobs=-1)]: Done 476 out of 491 | elapsed:    9.8s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 491 out of 491 | elapsed:   10.0s finished\n",
      "\n",
      "[2019-12-09 14:51:09] Features: 10/20 -- score: 0.8254970841522798[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    3.4s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    5.2s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:    8.3s\n",
      "[Parallel(n_jobs=-1)]: Done 490 out of 490 | elapsed:   10.3s finished\n",
      "\n",
      "[2019-12-09 14:51:19] Features: 11/20 -- score: 0.822609696866767[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:    8.2s\n",
      "[Parallel(n_jobs=-1)]: Done 489 out of 489 | elapsed:   10.3s finished\n",
      "\n",
      "[2019-12-09 14:51:30] Features: 12/20 -- score: 0.8221233873902014[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:    8.2s\n",
      "[Parallel(n_jobs=-1)]: Done 488 out of 488 | elapsed:   10.4s finished\n",
      "\n",
      "[2019-12-09 14:51:41] Features: 13/20 -- score: 0.8211599784734069[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    5.2s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:    8.5s\n",
      "[Parallel(n_jobs=-1)]: Done 487 out of 487 | elapsed:   10.7s finished\n",
      "\n",
      "[2019-12-09 14:51:52] Features: 14/20 -- score: 0.8197111970856743[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:    9.5s\n",
      "[Parallel(n_jobs=-1)]: Done 486 out of 486 | elapsed:   11.9s finished\n",
      "\n",
      "[2019-12-09 14:52:05] Features: 15/20 -- score: 0.8206745775544799[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:    9.2s\n",
      "[Parallel(n_jobs=-1)]: Done 485 out of 485 | elapsed:   11.8s finished\n",
      "\n",
      "[2019-12-09 14:52:17] Features: 16/20 -- score: 0.8187588935524625[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:    9.6s\n",
      "[Parallel(n_jobs=-1)]: Done 484 out of 484 | elapsed:   12.4s finished\n",
      "\n",
      "[2019-12-09 14:52:30] Features: 17/20 -- score: 0.8187663078095134[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    6.6s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   10.7s\n",
      "[Parallel(n_jobs=-1)]: Done 483 out of 483 | elapsed:   13.5s finished\n",
      "\n",
      "[2019-12-09 14:52:44] Features: 18/20 -- score: 0.8158770678487375[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    6.4s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=-1)]: Done 482 out of 482 | elapsed:   14.0s finished\n",
      "\n",
      "[2019-12-09 14:52:59] Features: 19/20 -- score: 0.8211618240366729[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   10.5s\n",
      "[Parallel(n_jobs=-1)]: Done 481 out of 481 | elapsed:   13.2s finished\n",
      "\n",
      "[2019-12-09 14:53:13] Features: 20/20 -- score: 0.8173128982896216"
     ]
    }
   ],
   "source": [
    "'''Step Foward Selection with DecisionTree Classification'''\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "sfs = SFS(DecisionTreeClassifier(random_state=42),\n",
    "         k_features = 20,\n",
    "          forward= True,\n",
    "          floating = False,\n",
    "          verbose= 2,\n",
    "          scoring= 'accuracy',\n",
    "          cv = 4,\n",
    "          n_jobs= -1\n",
    "         ).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy from SFS 0.8173128982896216\n"
     ]
    }
   ],
   "source": [
    "# '''Selected features and indexes'''\n",
    "print('Accuracy from SFS', sfs.k_score_)\n",
    "Selected_features_SFS = sfs.k_feature_names_\n",
    "Selected_indexes_SFS = list(sfs.k_feature_idx_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[221, 285, 336, 338, 378, 442, 475, 493]\n",
      "[28, 48, 64, 105, 128, 153, 241, 281, 318, 336, 338, 378, 442, 451, 453, 472, 475, 493]\n",
      "[48, 100, 128, 191, 195, 198, 279, 281, 295, 317, 318, 338, 340, 378, 404, 423, 435, 451, 472, 475]\n"
     ]
    }
   ],
   "source": [
    "'''Comparing the list of outputs from Lasso and Tree'''\n",
    "print(Selected_indexes_Lasso) #Using lasso\n",
    "print(Selected_indexes_Tree) #Using Tree\n",
    "print(Selected_indexes_SFS) #Using SFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[48, 128, 281, 318, 338, 378, 451, 472, 475]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Find commond indexes from Lasso, Tree and SFS'''\n",
    "# Common_indexes = []\n",
    "# for i in Selected_indexes_Tree:\n",
    "#     for j in Selected_indexes_Lasso:\n",
    "#         for k in Selected_indexes_SFS:\n",
    "#             if i == j == k:\n",
    "#                 Common_indexes.append(i)\n",
    "# Common_indexes\n",
    "\n",
    "'''Find commond indexes from Tree and SFS'''\n",
    "Common_indexes = []\n",
    "for i in Selected_indexes_Tree:\n",
    "    for j in Selected_indexes_SFS:\n",
    "        if i == j:\n",
    "            Common_indexes.append(i)\n",
    "Common_indexes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8269230769230769\n"
     ]
    }
   ],
   "source": [
    "'''Create a dataframe with just the above features and compute accuracy'''\n",
    "chosendf = X.iloc[:, Common_indexes]\n",
    "Rf_train, Rf_test, y_train, y_test = train_test_split(chosendf, y, test_size=0.2, random_state=42)\n",
    "rfc = DecisionTreeClassifier(random_state = 42)\n",
    "rfc.fit(Rf_train, y_train)\n",
    "rfc_predict = rfc.predict(Rf_test)\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y_test, rfc_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V49</th>\n",
       "      <th>V129</th>\n",
       "      <th>V282</th>\n",
       "      <th>V319</th>\n",
       "      <th>V339</th>\n",
       "      <th>V379</th>\n",
       "      <th>V452</th>\n",
       "      <th>V473</th>\n",
       "      <th>V476</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>443</td>\n",
       "      <td>500</td>\n",
       "      <td>481</td>\n",
       "      <td>503</td>\n",
       "      <td>339</td>\n",
       "      <td>443</td>\n",
       "      <td>480</td>\n",
       "      <td>480</td>\n",
       "      <td>641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>416</td>\n",
       "      <td>497</td>\n",
       "      <td>469</td>\n",
       "      <td>551</td>\n",
       "      <td>340</td>\n",
       "      <td>392</td>\n",
       "      <td>489</td>\n",
       "      <td>469</td>\n",
       "      <td>570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1772</th>\n",
       "      <td>526</td>\n",
       "      <td>475</td>\n",
       "      <td>567</td>\n",
       "      <td>446</td>\n",
       "      <td>479</td>\n",
       "      <td>563</td>\n",
       "      <td>470</td>\n",
       "      <td>473</td>\n",
       "      <td>431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>442</td>\n",
       "      <td>476</td>\n",
       "      <td>454</td>\n",
       "      <td>434</td>\n",
       "      <td>559</td>\n",
       "      <td>442</td>\n",
       "      <td>467</td>\n",
       "      <td>536</td>\n",
       "      <td>498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2213</th>\n",
       "      <td>419</td>\n",
       "      <td>476</td>\n",
       "      <td>532</td>\n",
       "      <td>443</td>\n",
       "      <td>480</td>\n",
       "      <td>416</td>\n",
       "      <td>468</td>\n",
       "      <td>531</td>\n",
       "      <td>446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1638</th>\n",
       "      <td>463</td>\n",
       "      <td>467</td>\n",
       "      <td>493</td>\n",
       "      <td>497</td>\n",
       "      <td>574</td>\n",
       "      <td>467</td>\n",
       "      <td>480</td>\n",
       "      <td>475</td>\n",
       "      <td>421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>517</td>\n",
       "      <td>450</td>\n",
       "      <td>444</td>\n",
       "      <td>410</td>\n",
       "      <td>829</td>\n",
       "      <td>535</td>\n",
       "      <td>469</td>\n",
       "      <td>555</td>\n",
       "      <td>282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130</th>\n",
       "      <td>486</td>\n",
       "      <td>485</td>\n",
       "      <td>528</td>\n",
       "      <td>456</td>\n",
       "      <td>488</td>\n",
       "      <td>500</td>\n",
       "      <td>471</td>\n",
       "      <td>530</td>\n",
       "      <td>407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294</th>\n",
       "      <td>513</td>\n",
       "      <td>482</td>\n",
       "      <td>516</td>\n",
       "      <td>521</td>\n",
       "      <td>419</td>\n",
       "      <td>536</td>\n",
       "      <td>486</td>\n",
       "      <td>412</td>\n",
       "      <td>547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>473</td>\n",
       "      <td>475</td>\n",
       "      <td>581</td>\n",
       "      <td>478</td>\n",
       "      <td>480</td>\n",
       "      <td>480</td>\n",
       "      <td>476</td>\n",
       "      <td>504</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2080 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      V49  V129  V282  V319  V339  V379  V452  V473  V476\n",
       "582   443   500   481   503   339   443   480   480   641\n",
       "48    416   497   469   551   340   392   489   469   570\n",
       "1772  526   475   567   446   479   563   470   473   431\n",
       "964   442   476   454   434   559   442   467   536   498\n",
       "2213  419   476   532   443   480   416   468   531   446\n",
       "...   ...   ...   ...   ...   ...   ...   ...   ...   ...\n",
       "1638  463   467   493   497   574   467   480   475   421\n",
       "1095  517   450   444   410   829   535   469   555   282\n",
       "1130  486   485   528   456   488   500   471   530   407\n",
       "1294  513   482   516   521   419   536   486   412   547\n",
       "860   473   475   581   478   480   480   476   504   336\n",
       "\n",
       "[2080 rows x 9 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Fresh re-load of dataset'''\n",
    "X = df.drop(['Class'], axis = 1)\n",
    "y = df['Class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train_ = X_train.iloc[:, Common_indexes]\n",
    "X_test_ = X_test.iloc[:, Common_indexes]\n",
    "X_train_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Features: 126/126"
     ]
    }
   ],
   "source": [
    "'''Pick 5 most useful features using Exhaustive Feature Selection'''\n",
    "from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS\n",
    "efs = EFS(DecisionTreeClassifier(random_state=42),\n",
    "         min_features= 5,\n",
    "          max_features= 5,\n",
    "          scoring='accuracy',\n",
    "          cv = 20,\n",
    "          n_jobs=-1\n",
    "         ).fit(X_train_, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highest accuracy acheived by EFS is 0.8270289661794517\n"
     ]
    }
   ],
   "source": [
    "print('The highest accuracy acheived by EFS is', efs.best_score_)  #Accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final selected features are, ['V129', 'V282', 'V319', 'V379', 'V476']\n"
     ]
    }
   ],
   "source": [
    "Selected_features_Final = list(efs.best_feature_names_)\n",
    "print('The final selected features are,',Selected_features_Final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128, 281, 318, 378, 475]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Selected_indexes_Final = get_index(Selected_features_Final)\n",
    "Selected_indexes_Final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep only 5 final features in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2600, 500)\n"
     ]
    }
   ],
   "source": [
    "'''Load data'''\n",
    "X = df.drop(['Class'], axis = 1)\n",
    "y = df['Class']\n",
    "\n",
    "scaler= StandardScaler().fit(X)\n",
    "X = scaler.transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X = pd.DataFrame(X)\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dimensions of inputdataset is:  (2600, 9)\n",
      "Accuracy:  0.8269230769230769\n"
     ]
    }
   ],
   "source": [
    "chosendf = X.iloc[:, Common_indexes]\n",
    "\n",
    "print(\"Final dimensions of inputdataset is: \", chosendf.shape)\n",
    "\n",
    "Rf_train, Rf_test, y_train, y_test = train_test_split(chosendf, y, test_size=0.2, random_state=42)\n",
    "rfc = DecisionTreeClassifier(random_state = 42)\n",
    "rfc.fit(Rf_train, y_train)\n",
    "rfc_predict = rfc.predict(Rf_test)\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y_test, rfc_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Performance measures for test set:')\n",
    "print('Accuracy:', accuracy_scores(y_test, rfc_predict))\n",
    "print('F1 Score:',f1_score(y_test, rfc_predict))\n",
    "print('Precision Score:',precision_score(y_test, rfc_predict))\n",
    "print('Recall Score:', recall_score(y_test, rfc_predict))\n",
    "print('Confusion Matrix:\\n', confusion_matrix(y_test, rfc_predict))\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
